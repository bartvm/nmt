import theano
import theano.tensor as tensor
import numpy
import copy
from data_iterator import WordPairIterator
from utils import *
from optimizers import *
from layers import *
from nmt import init_tparams
from unknown_nmt import contextwin, getmaskwin, get_mask_matrix, get_ctx_matrix, init_params, prepare_data;
from theano.sandbox.rng_mrg import MRG_RandomStreams as RandomStreams

def inspect_inputs(i, node, fn):
    print i, node, "input(s) value(s):", [input[0] for input in fn.inputs],

def inspect_outputs(i, node, fn):
    print "output(s) value(s):", [output[0] for output in fn.outputs]


def build_model(tparams, options):
    opt_ret = dict()

    trng = RandomStreams(1234)
    use_noise = theano.shared(numpy.float32(0.))

    # description string: #words x #samples
    x = tensor.matrix('x', dtype='int64')
    x_mask = tensor.matrix('x_mask', dtype='float32')
    y = tensor.matrix('y', dtype='int64')
    y_mask = tensor.matrix('y_mask', dtype='float32')
    unk_ctx = tensor.matrix('unk_ctx', dtype='int64') 

    # word embedding for forward rnn (source)
    emb = tparams['Wemb'][x.flatten()]
    n_ctx = unk_ctx.shape[0]
    unk_ctx_wrd_emb = tparams['Wemb'][unk_ctx.flatten()]
    unk_ctx_wrd_emb = unk_ctx_wrd_emb.reshape([n_ctx, options['ctx_len_emb']*options['dim_word_src']])
	
    unk_ctx_emb = get_layer('ff')[1](tparams, unk_ctx_wrd_emb, options,
				     prefix='ff_embedding', activ='tanh')
					    

    # for the backward rnn, we just need to invert x and x_mask
    xr = x[::-1]
    xr_mask = x_mask[::-1]
    n_timesteps = x.shape[0]
    n_timesteps_trg = y.shape[0]
    n_samples = x.shape[1]
    x_temp = x.flatten();    

    ones_vec = T.switch(T.eq(x_temp, 1), 1, 0)
    cum_sum = T.extra_ops.cumsum(ones_vec)
    final = T.switch(T.eq(x_temp, 1), cum_sum + options['n_words_src'] - 1, x_temp)
    c_word_embedding = concatenate([tparams['Wemb'], unk_ctx_emb], axis=0) 

    emb = c_word_embedding[final[:]];
    emb = emb.reshape([n_timesteps, n_samples, options['dim_word_src']])

    proj = get_layer(options['encoder'])[1](tparams, emb, options,
                                            prefix='encoder',
                                            mask=x_mask)
    # word embedding for backward rnn (source)



    embr = tparams['Wemb'][xr.flatten()]
    embr = embr.reshape([n_timesteps, n_samples, options['dim_word_src']])


    projr = get_layer(options['encoder'])[1](tparams, embr, options,
                                             prefix='encoder_r',
                                             mask=xr_mask)

    # context will be the concatenation of forward and backward rnns
    ctx = concatenate([proj[0], projr[0][::-1]], axis=proj[0].ndim-1)

    # mean of the context (across time) will be used to initialize decoder rnn
    ctx_mean = (ctx * x_mask[:, :, None]).sum(0) / x_mask.sum(0)[:, None]

    # or you can use the last state of forward + backward encoder rnns
    # ctx_mean = concatenate([proj[0][-1], projr[0][-1]], axis=proj[0].ndim-2)

    # initial decoder state
    init_state = get_layer('ff')[1](tparams, ctx_mean, options,
                                    prefix='ff_state', activ='tanh')

    # word embedding (target), we will shift the target sequence one time step
    # to the right. This is done because of the bi-gram connections in the
    # readout and decoder rnn. The first target will be all zeros and we will
    # not condition on the last output.
    emb = tparams['Wemb_dec'][y.flatten()]
    emb = emb.reshape([n_timesteps_trg, n_samples, options['dim_word_trg']])
    emb_shifted = tensor.zeros_like(emb)
    emb_shifted = tensor.set_subtensor(emb_shifted[1:], emb[:-1])
    emb = emb_shifted

    # decoder - pass through the decoder conditional gru with attention
    proj = get_layer(options['decoder'])[1](tparams, emb, options,
                                            prefix='decoder',
                                            mask=y_mask, context=ctx,
                                            context_mask=x_mask,
                                            one_step=False,
                                            init_state=init_state)
    # hidden states of the decoder gru
    proj_h = proj[0]

    # weighted averages of context, generated by attention module
    ctxs = proj[1]

    # weights (alignment matrix)
    opt_ret['dec_alphas'] = proj[2]

    # compute word probabilities
    logit_lstm = get_layer('ff')[1](tparams, proj_h, options,
                                    prefix='ff_logit_lstm', activ='linear')
    logit_prev = get_layer('ff')[1](tparams, emb, options,
                                    prefix='ff_logit_prev', activ='linear')
    logit_ctx = get_layer('ff')[1](tparams, ctxs, options,
                                   prefix='ff_logit_ctx', activ='linear')
    logit = tensor.tanh(logit_lstm+logit_prev+logit_ctx)
    if options['use_dropout']:
        logit = dropout_layer(logit, use_noise, trng)
    logit = get_layer('ff')[1](tparams, logit, options, 
                               prefix='ff_logit', activ='linear')
    logit_shp = logit.shape
    probs = tensor.nnet.softmax(logit.reshape([logit_shp[0]*logit_shp[1],
                                               logit_shp[2]]))

    # cost
    y_flat = y.flatten()
    y_flat_idx = tensor.arange(y_flat.shape[0]) * options['n_words'] + y_flat
    cost = -tensor.log(probs.flatten()[y_flat_idx])
    cost = cost.reshape([y.shape[0], y.shape[1]])
    cost = (cost * y_mask).sum(0)
    
    return trng, use_noise, x, x_mask, y, y_mask, opt_ret, cost, unk_ctx


def build_sampler(tparams, options, trng):
    x = tensor.matrix('x', dtype='int64')
    unk_ctx = tensor.matrix('unk_ctx', dtype='int64') 

    xr = x[::-1]
    n_timesteps = x.shape[0]
    n_samples = x.shape[1]
    n_ctx = unk_ctx.shape[0] 

    emb = tparams['Wemb'][x.flatten()]
    unk_ctx_wrd_emb = tparams['Wemb'][unk_ctx.flatten()]
    unk_ctx_wrd_emb = unk_ctx_wrd_emb.reshape([n_ctx, options['ctx_len_emb']*options['dim_word_src']])
    unk_ctx_emb = get_layer('ff')[1](tparams, unk_ctx_wrd_emb, options,
				     prefix='ff_embedding',
			             activ='tanh')
	
   # word embedding (source), forward and backward
    x_temp = x.flatten();    
    ones_vec = T.switch(T.eq(x_temp, 1), 1, 0)
    cum_sum = T.extra_ops.cumsum(ones_vec)
    final = T.switch(T.eq(x_temp, 1), cum_sum + options['n_words_src'] - 1, x_temp)
    c_word_embedding = concatenate([tparams['Wemb'], unk_ctx_emb], axis=0)
    emb = c_word_embedding[final[:]];  
	     
    emb = emb.reshape([n_timesteps, n_samples, options['dim_word_src']])
    embr = tparams['Wemb'][xr.flatten()]
    embr = embr.reshape([n_timesteps, n_samples, options['dim_word_src']])

    # encoder
    proj = get_layer(options['encoder'])[1](tparams, emb, options,
                                            prefix='encoder')
    projr = get_layer(options['encoder'])[1](tparams, embr, options,
                                             prefix='encoder_r')

    # concatenate forward and backward rnn hidden states
    ctx = concatenate([proj[0], projr[0][::-1]], axis=proj[0].ndim-1)

    # get the input for decoder rnn initializer mlp
    ctx_mean = ctx.mean(0)
    # ctx_mean = concatenate([proj[0][-1],projr[0][-1]], axis=proj[0].ndim-2)
    init_state = get_layer('ff')[1](tparams, ctx_mean, options,
                                    prefix='ff_state', activ='tanh')

    print 'Building f_init...',
    outs = [init_state, ctx]
    f_init = theano.function([x, unk_ctx], outs, name='f_init', profile=profile)
    print 'Done'

   
    get_emb = theano.function([x, unk_ctx], [emb], on_unused_input='ignore')
    get_temp = theano.function([x, unk_ctx], [ctx], on_unused_input='ignore')
    

    return f_init, get_emb
    
 #   get_final = theano.function([x, unk_ctx],[final], on_unused_input='ignore')
 #   get_triple = theano.function([x, unk_ctx],[unk_ctx_emb], on_unused_input='ignore')
 #   get_ctx = theano.function([x, unk_ctx], [ctx], on_unused_input='ignore')

 #   return get_final, get_triple, get_emb, get_ctx


if __name__ == '__main__':
    src_data = '/data/lisatmp3/nmt/data/europarl-v7.fr-en.fr.tok'
    trg_data = '/data/lisatmp3/nmt/data/europarl-v7.fr-en.en.tok'
    src_dict = '/data/lisatmp3/nmt/data/europarl-v7.fr-en.fr.tok.pkl'
    trg_dict = '/data/lisatmp3/nmt/data/europarl-v7.fr-en.en.tok.pkl'

    datasets = [src_data, trg_data]
    dictionaries = [src_dict, trg_dict]

    batch_size = 5
    maxlen = 15
    n_words = 100
    n_words_src, n_words_target = 40,100
    dim_word_src = 10
    dim_word_trg = 100
    dim = 1000
    encoder='gru'
    decoder='gru_cond'
    patience=10
    max_epochs=5000
    use_dropout = False 
    ctx_len_emb = 5
    model_options = locals().copy()
    params = init_params(model_options)
    tparams = init_tparams(params)
    train = WordPairIterator(src_data, trg_data, src_dict, trg_dict,
                        n_words_source=n_words_src,
                        n_words_target=n_words_target,
                        batch_size=batch_size, maxlen=maxlen)
     
    trng, use_noise, x, x_mask, y, y_mask, opt_ret, cost, unk_ctx = build_model(tparams, model_options);
    
    inps = [x, x_mask, y, y_mask]
    f_init, get_emb = build_sampler(tparams, model_options, trng)

    print 'Ready'
    for x, y in train:
        x, x_mask, y, y_mask, unk_ctx = prepare_data(x, y, maxlen=25)
        break
     
    print unk_ctx.shape[0]
#    triple = get_triple(x, unk_ctx)
#    final = get_final(x, unk_ctx)
#    print final
#    ctx = get_ctx(x, unk_ctx)
#    print triple[0].shape
#    print len(ctx[0])

    emb = get_emb(x, unk_ctx)
    print f_init(x, unk_ctx)
